{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-05-11T19:55:38.225600Z","iopub.execute_input":"2024-05-11T19:55:38.226635Z","iopub.status.idle":"2024-05-11T19:55:42.170033Z","shell.execute_reply.started":"2024-05-11T19:55:38.226595Z","shell.execute_reply":"2024-05-11T19:55:42.168909Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\nprint('Train shape', train.shape )\ntrain","metadata":{"execution":{"iopub.status.busy":"2024-05-11T19:55:42.174343Z","iopub.execute_input":"2024-05-11T19:55:42.175270Z","iopub.status.idle":"2024-05-11T19:55:42.505841Z","shell.execute_reply.started":"2024-05-11T19:55:42.175237Z","shell.execute_reply":"2024-05-11T19:55:42.504792Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Train shape (106800, 15)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"            eeg_id  eeg_sub_id  eeg_label_offset_seconds  spectrogram_id  \\\n0       1628180742           0                       0.0          353733   \n1       1628180742           1                       6.0          353733   \n2       1628180742           2                       8.0          353733   \n3       1628180742           3                      18.0          353733   \n4       1628180742           4                      24.0          353733   \n...            ...         ...                       ...             ...   \n106795   351917269           6                      12.0      2147388374   \n106796   351917269           7                      14.0      2147388374   \n106797   351917269           8                      16.0      2147388374   \n106798   351917269           9                      18.0      2147388374   \n106799   351917269          10                      20.0      2147388374   \n\n        spectrogram_sub_id  spectrogram_label_offset_seconds    label_id  \\\n0                        0                               0.0   127492639   \n1                        1                               6.0  3887563113   \n2                        2                               8.0  1142670488   \n3                        3                              18.0  2718991173   \n4                        4                              24.0  3080632009   \n...                    ...                               ...         ...   \n106795                   6                              12.0  4195677307   \n106796                   7                              14.0   290896675   \n106797                   8                              16.0   461435451   \n106798                   9                              18.0  3786213131   \n106799                  10                              20.0  3642716176   \n\n        patient_id expert_consensus  seizure_vote  lpd_vote  gpd_vote  \\\n0            42516          Seizure             3         0         0   \n1            42516          Seizure             3         0         0   \n2            42516          Seizure             3         0         0   \n3            42516          Seizure             3         0         0   \n4            42516          Seizure             3         0         0   \n...            ...              ...           ...       ...       ...   \n106795       10351             LRDA             0         0         0   \n106796       10351             LRDA             0         0         0   \n106797       10351             LRDA             0         0         0   \n106798       10351             LRDA             0         0         0   \n106799       10351             LRDA             0         0         0   \n\n        lrda_vote  grda_vote  other_vote  \n0               0          0           0  \n1               0          0           0  \n2               0          0           0  \n3               0          0           0  \n4               0          0           0  \n...           ...        ...         ...  \n106795          3          0           0  \n106796          3          0           0  \n106797          3          0           0  \n106798          3          0           0  \n106799          3          0           0  \n\n[106800 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eeg_id</th>\n      <th>eeg_sub_id</th>\n      <th>eeg_label_offset_seconds</th>\n      <th>spectrogram_id</th>\n      <th>spectrogram_sub_id</th>\n      <th>spectrogram_label_offset_seconds</th>\n      <th>label_id</th>\n      <th>patient_id</th>\n      <th>expert_consensus</th>\n      <th>seizure_vote</th>\n      <th>lpd_vote</th>\n      <th>gpd_vote</th>\n      <th>lrda_vote</th>\n      <th>grda_vote</th>\n      <th>other_vote</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1628180742</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>353733</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>127492639</td>\n      <td>42516</td>\n      <td>Seizure</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1628180742</td>\n      <td>1</td>\n      <td>6.0</td>\n      <td>353733</td>\n      <td>1</td>\n      <td>6.0</td>\n      <td>3887563113</td>\n      <td>42516</td>\n      <td>Seizure</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1628180742</td>\n      <td>2</td>\n      <td>8.0</td>\n      <td>353733</td>\n      <td>2</td>\n      <td>8.0</td>\n      <td>1142670488</td>\n      <td>42516</td>\n      <td>Seizure</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1628180742</td>\n      <td>3</td>\n      <td>18.0</td>\n      <td>353733</td>\n      <td>3</td>\n      <td>18.0</td>\n      <td>2718991173</td>\n      <td>42516</td>\n      <td>Seizure</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1628180742</td>\n      <td>4</td>\n      <td>24.0</td>\n      <td>353733</td>\n      <td>4</td>\n      <td>24.0</td>\n      <td>3080632009</td>\n      <td>42516</td>\n      <td>Seizure</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>106795</th>\n      <td>351917269</td>\n      <td>6</td>\n      <td>12.0</td>\n      <td>2147388374</td>\n      <td>6</td>\n      <td>12.0</td>\n      <td>4195677307</td>\n      <td>10351</td>\n      <td>LRDA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>106796</th>\n      <td>351917269</td>\n      <td>7</td>\n      <td>14.0</td>\n      <td>2147388374</td>\n      <td>7</td>\n      <td>14.0</td>\n      <td>290896675</td>\n      <td>10351</td>\n      <td>LRDA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>106797</th>\n      <td>351917269</td>\n      <td>8</td>\n      <td>16.0</td>\n      <td>2147388374</td>\n      <td>8</td>\n      <td>16.0</td>\n      <td>461435451</td>\n      <td>10351</td>\n      <td>LRDA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>106798</th>\n      <td>351917269</td>\n      <td>9</td>\n      <td>18.0</td>\n      <td>2147388374</td>\n      <td>9</td>\n      <td>18.0</td>\n      <td>3786213131</td>\n      <td>10351</td>\n      <td>LRDA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>106799</th>\n      <td>351917269</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>2147388374</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>3642716176</td>\n      <td>10351</td>\n      <td>LRDA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>106800 rows Ã— 15 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"TARGETS = train.columns[-6:]\n\ntrain_17k = train.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg(\n    {'spectrogram_id':'first','spectrogram_label_offset_seconds':'min'})\ntrain_17k.columns = ['spec_id','min']\n\ntmp = train.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg(\n    {'spectrogram_label_offset_seconds':'max'})\ntrain_17k['max'] = tmp\n\ntmp = train.groupby('eeg_id')[['patient_id']].agg('first')\ntrain_17k['patient_id'] = tmp\n\ntmp = train.groupby('eeg_id')[TARGETS].agg('sum')\nfor t in TARGETS:\n    train_17k[t] = tmp[t].values\n    \ny_data = train_17k[TARGETS].values\ny_data = y_data / y_data.sum(axis=1,keepdims=True)\ntrain_17k[TARGETS] = y_data\n\ntmp = train.groupby('eeg_id')[['expert_consensus']].agg('first')\ntrain['target'] = tmp\n\ntrain_17k = train_17k.reset_index()\nprint('Train non-overlapp eeg_id shape:', train_17k.shape )\ntrain_17k","metadata":{"execution":{"iopub.status.busy":"2024-05-11T19:55:42.507503Z","iopub.execute_input":"2024-05-11T19:55:42.507842Z","iopub.status.idle":"2024-05-11T19:55:42.632855Z","shell.execute_reply.started":"2024-05-11T19:55:42.507813Z","shell.execute_reply":"2024-05-11T19:55:42.632007Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Train non-overlapp eeg_id shape: (17089, 11)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"           eeg_id     spec_id     min     max  patient_id  seizure_vote  \\\n0          568657   789577333     0.0    16.0       20654           0.0   \n1          582999  1552638400     0.0    38.0       20230           0.0   \n2          642382    14960202  1008.0  1032.0        5955           0.0   \n3          751790   618728447   908.0   908.0       38549           0.0   \n4          778705    52296320     0.0     0.0       40955           0.0   \n...           ...         ...     ...     ...         ...           ...   \n17084  4293354003  1188113564     0.0     0.0       16610           0.0   \n17085  4293843368  1549502620     0.0     0.0       15065           0.0   \n17086  4294455489  2105480289     0.0     0.0          56           0.0   \n17087  4294858825   657299228     0.0    12.0        4312           0.0   \n17088  4294958358   260520016  2508.0  2508.0       25986           0.0   \n\n       lpd_vote  gpd_vote  lrda_vote  grda_vote  other_vote  \n0      0.000000      0.25   0.000000   0.166667    0.583333  \n1      0.857143      0.00   0.071429   0.000000    0.071429  \n2      0.000000      0.00   0.000000   0.000000    1.000000  \n3      0.000000      1.00   0.000000   0.000000    0.000000  \n4      0.000000      0.00   0.000000   0.000000    1.000000  \n...         ...       ...        ...        ...         ...  \n17084  0.000000      0.00   0.000000   0.500000    0.500000  \n17085  0.000000      0.00   0.000000   0.500000    0.500000  \n17086  0.000000      0.00   0.000000   0.000000    1.000000  \n17087  0.000000      0.00   0.000000   0.066667    0.933333  \n17088  0.000000      0.00   0.000000   0.000000    1.000000  \n\n[17089 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eeg_id</th>\n      <th>spec_id</th>\n      <th>min</th>\n      <th>max</th>\n      <th>patient_id</th>\n      <th>seizure_vote</th>\n      <th>lpd_vote</th>\n      <th>gpd_vote</th>\n      <th>lrda_vote</th>\n      <th>grda_vote</th>\n      <th>other_vote</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>568657</td>\n      <td>789577333</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>20654</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.25</td>\n      <td>0.000000</td>\n      <td>0.166667</td>\n      <td>0.583333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>582999</td>\n      <td>1552638400</td>\n      <td>0.0</td>\n      <td>38.0</td>\n      <td>20230</td>\n      <td>0.0</td>\n      <td>0.857143</td>\n      <td>0.00</td>\n      <td>0.071429</td>\n      <td>0.000000</td>\n      <td>0.071429</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>642382</td>\n      <td>14960202</td>\n      <td>1008.0</td>\n      <td>1032.0</td>\n      <td>5955</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>751790</td>\n      <td>618728447</td>\n      <td>908.0</td>\n      <td>908.0</td>\n      <td>38549</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>778705</td>\n      <td>52296320</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>40955</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17084</th>\n      <td>4293354003</td>\n      <td>1188113564</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>16610</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>17085</th>\n      <td>4293843368</td>\n      <td>1549502620</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15065</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>17086</th>\n      <td>4294455489</td>\n      <td>2105480289</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>56</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>17087</th>\n      <td>4294858825</td>\n      <td>657299228</td>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>4312</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.066667</td>\n      <td>0.933333</td>\n    </tr>\n    <tr>\n      <th>17088</th>\n      <td>4294958358</td>\n      <td>260520016</td>\n      <td>2508.0</td>\n      <td>2508.0</td>\n      <td>25986</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>17089 rows Ã— 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"ids = train.eeg_id.unique()\nsample_ids = ids[0:5]\nsample_ids","metadata":{"execution":{"iopub.status.busy":"2024-05-11T19:55:42.635160Z","iopub.execute_input":"2024-05-11T19:55:42.635665Z","iopub.status.idle":"2024-05-11T19:55:42.645865Z","shell.execute_reply.started":"2024-05-11T19:55:42.635628Z","shell.execute_reply":"2024-05-11T19:55:42.644668Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"array([1628180742, 2277392603,  722738444,  387987538, 2175806584])"},"metadata":{}}]},{"cell_type":"code","source":"import pywt\n\ndef maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef denoise(x, wavelet='db8', level=1):\n    #x = x.iloc[:, 1:-7]\n    ret = {key:[] for key in x.columns}\n    \n    for pos in x.columns:\n        coeff = pywt.wavedec(x[pos], wavelet, mode=\"per\")\n        sigma = (1/0.6745) * maddest(coeff[-level])\n\n        uthresh = sigma * np.sqrt(2*np.log(len(x)))\n        coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n        ret[pos]=pywt.waverec(coeff, wavelet, mode='per')\n    \n    return pd.DataFrame(ret)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T19:55:42.647088Z","iopub.execute_input":"2024-05-11T19:55:42.647509Z","iopub.status.idle":"2024-05-11T19:55:42.879046Z","shell.execute_reply.started":"2024-05-11T19:55:42.647481Z","shell.execute_reply":"2024-05-11T19:55:42.877873Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# DWT\nfrom pywt import wavedec\n\ndef wavelet_decompose_channels(data, level, output=False):\n  # take every x number of points using numpy's slicing (start:stop:step)\n    data = data[0::2]\n\n    data.columns.name='channel'\n\n    # transpose the data\n    data_t = data.transpose()\n\n    # get the wavelet coefficients at each level in a list\n    coeffs_list = wavedec(data_t.values, wavelet='db4', level=level)\n    #print(len(coeffs_list))\n\n    # make a list of the component names (later column rows)\n    nums = list(range(1,level+1))\n    names=[]\n    for num in nums:\n        names.append('D' + str(num))\n    names.append('A' + str(nums[-1]))\n\n  # reverse the names so it counts down\n    names = names[::-1]  \n    #print(names)\n\n    i = 0\n    wavelets = pd.DataFrame()\n    for i in range(1, len(coeffs_list)):\n    #for i, array in enumerate(coeffs_list):\n        #print(i)\n        array = coeffs_list[i]\n        # turn into a dataframe and transpose\n        level_df = pd.DataFrame(array)\n        level_df.index = data.columns\n        level_df['level'] = names[i]\n        level_df= level_df.set_index('level', append=True)\n        level_df=level_df.T\n        # add the next levels df to another column\n        wavelets = pd.concat([wavelets,level_df], axis=1, sort=True)\n\n    # sort values along the channels\n    wavelets = wavelets.sort_values(['channel', 'level'], axis=1)\n\n    wavelets_cleaned = wavelets.dropna()\n\n    return wavelets_cleaned\n\n# usage\n#dwt_wavelets = wavelet_decompose_channels(denoised_eeg_data, level=5, output=True)\n#dwt_wavelets","metadata":{"execution":{"iopub.status.busy":"2024-05-11T19:55:42.880421Z","iopub.execute_input":"2024-05-11T19:55:42.880893Z","iopub.status.idle":"2024-05-11T19:55:42.892958Z","shell.execute_reply.started":"2024-05-11T19:55:42.880862Z","shell.execute_reply":"2024-05-11T19:55:42.891614Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import entropy\n\ndef MAV(data):\n    # Initialize an empty DataFrame to store the means for each channel\n    means = pd.DataFrame(index=data.index)\n\n    # Iterate over each channel and calculate the mean across 'D1' to 'D5'\n    for channel in data.columns.get_level_values(0).unique():\n    # Calculate the mean for the current channel\n        means[channel] = data[channel].abs().mean(axis=1)\n    \n    means.columns = [f\"{col}_DT_MAV\" for col in means.columns]\n    \n    return means\n\n\ndef MAVP(data):\n    # Initialize an empty DataFrame to store the means for each channel\n    means_abs = pd.DataFrame(index=data.index)\n\n    # Iterate over each channel and calculate the mean across 'D1' to 'D5'\n    for channel in data.columns.get_level_values(0).unique():\n    # Calculate the mean for the current channel\n        means_abs[channel] = (data[channel]**2).mean(axis = 1)\n    \n    means_abs.columns = [f\"{col}_DT_MAVP\" for col in means_abs.columns]\n    \n    return means_abs\n\n\n\ndef std_val(data):\n    # Initialize an empty DataFrame to store the means for each channel\n    std_vals = pd.DataFrame(index=data.index)\n\n    # Iterate over each channel and calculate the mean across 'D1' to 'D5'\n    for channel in data.columns.get_level_values(0).unique():\n    # Calculate the mean for the current channel\n        std_vals[channel] = data[channel].std(axis = 1)\n    \n    std_vals.columns = [f\"{col}_DT_STD\" for col in std_vals.columns]\n    \n    return std_vals\n\n\ndef var_val(data):\n    # Initialize an empty DataFrame to store the means for each channel\n    var = pd.DataFrame(index=data.index)\n\n    # Iterate over each channel and calculate the mean across 'D1' to 'D5'\n    for channel in data.columns.get_level_values(0).unique():\n    # Calculate the mean for the current channel\n        var[channel] = data[channel].var(axis = 1)\n    \n    var.columns = [f\"{col}_DT_VAR\" for col in var.columns]\n    \n    return var\n\n\ndef ratio_channels(epoch_data):\n    # Initialize an empty DataFrame to store the ratio values for each pair of channels\n    ratio_data = pd.DataFrame(index=epoch_data.index)\n    \n    # Iterate over each pair of adjacent channels\n    for i in range(len(epoch_data.columns) - 1):\n        channel1 = epoch_data.columns[i]\n        channel2 = epoch_data.columns[i + 1]\n        \n        # Calculate the ratio between the values of the two channels\n        ratio_data[f\"{channel1}-{channel2}_Ratio\"] = epoch_data[channel1] / epoch_data[channel2]\n    \n    return ratio_data\n\n\n\ndef shannon_entropy(data):\n    # Initialize an empty DataFrame to store the entropy values for each channel\n    entropy_df = pd.DataFrame(index=data.index)\n\n    # Iterate over each channel and calculate the entropy across 'D1' to 'D5'\n    for channel in data.columns.get_level_values(0).unique():\n        # Calculate the entropy for the current channel\n        entropy_values = data[channel].apply(lambda x: entropy(x.abs(), base=2), axis=1)\n        entropy_df[channel + '_entropy'] = entropy_values\n\n    return entropy_df","metadata":{"execution":{"iopub.status.busy":"2024-05-11T19:55:42.895344Z","iopub.execute_input":"2024-05-11T19:55:42.895856Z","iopub.status.idle":"2024-05-11T19:55:43.224470Z","shell.execute_reply.started":"2024-05-11T19:55:42.895814Z","shell.execute_reply":"2024-05-11T19:55:43.223342Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def eeg_feature(data):\n    \n    wavelet_mean = MAV(data)\n    wavelet_meanabs = MAVP(data)\n    wavelet_std = std_val(data)\n    wavelet_var = var_val(data)\n    wavelet_ratio = ratio_channels(data)\n    wavelet_entropy = shannon_entropy(data)\n    \n    wavelet_statistics = pd.concat([data, wavelet_mean, wavelet_meanabs, wavelet_std, wavelet_var, wavelet_ratio, wavelet_entropy], axis=1)\n    \n    wavelet_statistics.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in wavelet_statistics.columns]\n\n    \n    features_to_drop = ['C4_D1', 'F3_D1', 'F4_D1', 'F7_D1', 'F8_D1', 'Fp2_D1', 'Fz_D1', 'O2_D1', 'T3_D1', 'T4_D1', 'T5_D1', 'T6_D1', \"('Fp2', 'D1')-('Fp2', 'D2')_Ratio\", 'C3_entropy', 'C4_entropy', 'Cz_entropy', 'EKG_entropy', 'F3_entropy', 'F4_entropy', 'F7_entropy', 'F8_entropy', 'Fp1_entropy', 'Fp2_entropy', 'Fz_entropy', 'O1_entropy', 'O2_entropy', 'P3_entropy', 'P4_entropy', 'Pz_entropy', 'T3_entropy', 'T4_entropy', 'T5_entropy', 'T6_entropy']\n\n    # Drop the specified features from the DataFrame\n    wavelet_statistics.drop(columns=features_to_drop, inplace=True)\n    \n    return wavelet_statistics\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T19:55:43.225802Z","iopub.execute_input":"2024-05-11T19:55:43.226382Z","iopub.status.idle":"2024-05-11T19:55:43.236024Z","shell.execute_reply.started":"2024-05-11T19:55:43.226350Z","shell.execute_reply":"2024-05-11T19:55:43.234853Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def feature_eng_eeg(parquet_path, eegid, display=False):\n        \n    GET_ROW = 0\n\n    row = train.iloc[GET_ROW]\n\n    eeg = pd.read_parquet(parquet_path)\n    \n    # Denoise\n    denoised_eeg_data = denoise(eeg, wavelet='db8') \n    \n    # DWT\n    dwt_wavelets = wavelet_decompose_channels(denoised_eeg_data, level=5, output=True)\n    \n    # getting all features\n    eegid_data_features = eeg_feature(dwt_wavelets)\n    \n    print(f\"features done {eeg_id}\")\n    \n    return eegid_data_features","metadata":{"execution":{"iopub.status.busy":"2024-05-11T19:55:49.040857Z","iopub.execute_input":"2024-05-11T19:55:49.041285Z","iopub.status.idle":"2024-05-11T19:55:49.048582Z","shell.execute_reply.started":"2024-05-11T19:55:49.041252Z","shell.execute_reply":"2024-05-11T19:55:49.047452Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"%%time\nPATH = '/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/'\n\nEEG_IDS = sample_ids\nall_eegs = {}\n# for i,eeg_id in tqdm(enumerate(EEG_IDS)): # use this when in kaggle notebook\nfor i,eeg_id in enumerate(EEG_IDS):         # use this when saving output\n    if (i%100==0)&(i!=0): \n        print(i,', ',end='')\n        \n    \n    # CREATE FEATURES FROM EEG PARQUET\n    all_features_data = feature_eng_eeg(f'{PATH}{eeg_id}.parquet',eeg_id)\n    # Remove rows with NaN values in feature data\n    clean_features_data = all_features_data.dropna()\n    \n    if clean_features_data.empty:\n        continue \n    \n    \n    # GET TARGET LABELS FOR SPECIFIC EEF_ID\n    train_17k_eegid = train_17k[train_17k['eeg_id'] == eeg_id]\n    target_labels = train_17k_eegid.iloc[0][['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']]\n\n    # SAVE \n    all_eegs[eeg_id] = (clean_features_data, target_labels)\n    \n\n   \n# # SAVE EEG SPECTROGRAM DICTIONARY\n#np.save('/kaggle/working/full_dataset',all_eegs)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T19:55:49.407011Z","iopub.execute_input":"2024-05-11T19:55:49.407396Z","iopub.status.idle":"2024-05-11T19:55:53.148225Z","shell.execute_reply.started":"2024-05-11T19:55:49.407367Z","shell.execute_reply":"2024-05-11T19:55:53.146829Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"features done 1628180742\nfeatures done 2277392603\nfeatures done 722738444\nfeatures done 387987538\nfeatures done 2175806584\nCPU times: user 3.48 s, sys: 41.6 ms, total: 3.52 s\nWall time: 3.73 s\n","output_type":"stream"}]},{"cell_type":"code","source":"len(all_eegs[2175806584][0].columns)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T19:55:53.150405Z","iopub.execute_input":"2024-05-11T19:55:53.150956Z","iopub.status.idle":"2024-05-11T19:55:53.158513Z","shell.execute_reply.started":"2024-05-11T19:55:53.150907Z","shell.execute_reply":"2024-05-11T19:55:53.157317Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"266"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-11T20:10:50.480485Z","iopub.execute_input":"2024-05-11T20:10:50.480902Z","iopub.status.idle":"2024-05-11T20:10:50.487478Z","shell.execute_reply.started":"2024-05-11T20:10:50.480870Z","shell.execute_reply":"2024-05-11T20:10:50.486114Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class EEGDataset(Dataset):\n    def __init__(self, data_dict):\n        # Initialize empty lists to hold concatenated data and targets\n        concatenated_data = []\n        concatenated_targets = []\n\n        # Loop over each item in the dictionary to concatenate data\n        for _, (df, target) in data_dict.items():\n            # Convert the dataframe to a tensor and append to list\n            eeg_data_tensor = torch.tensor(df.values, dtype=torch.float32)\n            target_tensor = torch.tensor(target, dtype=torch.float32).repeat(len(df), 1)  # Repeat target for each row in df\n            concatenated_data.append(eeg_data_tensor)\n            concatenated_targets.append(target_tensor)\n\n        # Concatenate all data tensors and target tensors\n        self.data = torch.cat(concatenated_data, dim=0)\n        self.targets = torch.cat(concatenated_targets, dim=0)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        # Return a single data point and its target\n        return self.data[index], self.targets[index]","metadata":{"execution":{"iopub.status.busy":"2024-05-11T21:42:00.865778Z","iopub.execute_input":"2024-05-11T21:42:00.866516Z","iopub.status.idle":"2024-05-11T21:42:00.877546Z","shell.execute_reply.started":"2024-05-11T21:42:00.866482Z","shell.execute_reply":"2024-05-11T21:42:00.876499Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"dataset = EEGDataset(all_eegs)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Example iteration over the DataLoader\nfor eeg_data, targets in dataloader:\n    print(eeg_data.shape)  # Each batch may have different sequence lengths but the same number of features\n    print(targets.shape)\n    break;","metadata":{"execution":{"iopub.status.busy":"2024-05-11T22:17:03.131352Z","iopub.execute_input":"2024-05-11T22:17:03.131738Z","iopub.status.idle":"2024-05-11T22:17:03.144717Z","shell.execute_reply.started":"2024-05-11T22:17:03.131695Z","shell.execute_reply":"2024-05-11T22:17:03.143587Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"torch.Size([32, 266])\ntorch.Size([32, 6])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/2157899305.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  target_tensor = torch.tensor(target, dtype=torch.float32).repeat(len(df), 1)  # Repeat target for each row in df\n","output_type":"stream"}]},{"cell_type":"code","source":"class CNNModel(nn.Module):\n    def __init__(self, num_features, num_classes):\n        super(CNNModel, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n            nn.LeakyReLU(),\n            nn.Conv1d(64, 64, kernel_size=3, padding=1),\n            nn.LeakyReLU(),\n            nn.MaxPool1d(2),\n            nn.BatchNorm1d(64),\n\n            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n            nn.LeakyReLU(),\n            nn.Conv1d(128, 128, kernel_size=3, padding=1),\n            nn.LeakyReLU(),\n            nn.MaxPool1d(2),\n            nn.BatchNorm1d(128)\n            \n            )\n        \n        # Calculate the number of features after all conv and pooling layers to correctly size the input to the linear layer\n        self._to_linear = None\n        self._num_features(num_features)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(self._to_linear, 512),\n            nn.LeakyReLU(),\n            nn.Linear(512, num_classes),\n            nn.Softmax(dim=1)\n        )\n\n    def _num_features(self, size):\n        # Forward pass on dummy data to calculate flat features after conv layers\n        inputs = torch.rand(1, 1, size)\n        outputs = self.features(inputs)\n        self._to_linear = int(outputs.numel() / outputs.shape[0])\n\n    def forward(self, x):\n        # Assuming x is of shape (batch, features), we unsqueeze to add channel dimension\n        x = x.unsqueeze(1)  # Shape becomes (batch, 1, features)\n        x = self.features(x)\n        x = x.view(x.size(0), -1)  # Flatten the output for the classifier\n        x = self.classifier(x)\n        return x\n\n# Example usage:\nmodel = CNNModel(num_features=266, num_classes=6)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:05:25.058630Z","iopub.execute_input":"2024-05-11T23:05:25.059081Z","iopub.status.idle":"2024-05-11T23:05:25.117953Z","shell.execute_reply.started":"2024-05-11T23:05:25.059047Z","shell.execute_reply":"2024-05-11T23:05:25.116883Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"# Assuming DWT and statistical features are loaded as (batch_size, num_features, sequence_length)\n# For example, let's assume 10 statistical features over 100 time intervals\n#model = CNNModel(num_features=266, num_classes=6)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\ncriterion = torch.nn.KLDivLoss(reduction=\"batchmean\")  # Or another appropriate loss function\nepochs = 10","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:05:45.577364Z","iopub.execute_input":"2024-05-11T23:05:45.577787Z","iopub.status.idle":"2024-05-11T23:05:45.584834Z","shell.execute_reply.started":"2024-05-11T23:05:45.577757Z","shell.execute_reply":"2024-05-11T23:05:45.583587Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(dataloader):\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        #print(labels.shape)\n\n        # Forward pass\n        outputs = model(inputs)\n        #print(outputs.shape)\n    \n        # Compute the loss\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Print statistics\n        running_loss += loss.item()\n        if i % 10 == 9:    # print every 10 mini-batches\n            print(f'Epoch {epoch + 1}, Batch {i + 1}: Loss: {running_loss / 10:.4f}')\n            running_loss = 0.0\n\n            \nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:05:46.077277Z","iopub.execute_input":"2024-05-11T23:05:46.077659Z","iopub.status.idle":"2024-05-11T23:05:53.808058Z","shell.execute_reply.started":"2024-05-11T23:05:46.077630Z","shell.execute_reply":"2024-05-11T23:05:53.806284Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"Epoch 1, Batch 10: Loss: nan\nEpoch 1, Batch 20: Loss: nan\nEpoch 1, Batch 30: Loss: nan\nEpoch 2, Batch 10: Loss: nan\nEpoch 2, Batch 20: Loss: nan\nEpoch 2, Batch 30: Loss: nan\nEpoch 3, Batch 10: Loss: nan\nEpoch 3, Batch 20: Loss: nan\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[130], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 19\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Print statistics\u001b[39;00m\n\u001b[1;32m     22\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-11T18:32:58.353460Z","iopub.execute_input":"2024-05-11T18:32:58.354922Z","iopub.status.idle":"2024-05-11T18:33:01.278951Z","shell.execute_reply.started":"2024-05-11T18:32:58.354875Z","shell.execute_reply":"2024-05-11T18:33:01.277776Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(epochs):\n    for eeg_data, targets in eeg_dataloader:\n        print(eeg_data.shape)\n        optimizer.zero_grad()\n        outputs = model(eeg_data)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch {epoch+1}, Loss: {loss.item()}')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T18:36:12.920688Z","iopub.execute_input":"2024-05-11T18:36:12.921113Z","iopub.status.idle":"2024-05-11T18:36:13.097371Z","shell.execute_reply.started":"2024-05-11T18:36:12.921081Z","shell.execute_reply":"2024-05-11T18:36:13.095459Z"},"trusted":true},"execution_count":26,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eeg_data, targets \u001b[38;5;129;01min\u001b[39;00m eeg_dataloader:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(eeg_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\nKeyError: 0\n"],"ename":"KeyError","evalue":"Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\nKeyError: 0\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='kullback_leibler_divergence', metrics=['accuracy'])\n\n# 3. Train the model\n# Example:\n# Train the model using the SWT dataset\nhistory = model.fit(dwt_wavelets_features, replicated_labels_df, epochs=10, batch_size=32, validation_split=0.2)\n\n# 4. Evaluate the model\n# Example:\n# Evaluate the trained model on the test set\ntest_loss, test_accuracy = model.evaluate(X_test_normalized, y_test)\nprint(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n\n# Optional: Visualize training history\n# Example:\n# Plot training and validation accuracy over epochs\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}